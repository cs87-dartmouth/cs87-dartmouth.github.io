<meta charset="utf-8"><!-- -*- markdown -*- -->
<style class="fallback">body{visibility:hidden}</style>

**Assignment 3: Sampling Materials**

In this assignment, we will take a deep dive into implementing a Monte Carlo renderer. Monte Carlo is a powerful technique for approximating integrals, and it is the dominant technique for rendering images in the movie industry and, increasingly, video games.

It turns out that the renderer you have been writing so far already did Monte Carlo integration in secret - but it was cleverly brushed under the table by Pete Shirley's books. In this assignment, you will redesign your renderer to make the Monte Carlo component explicit. This will allow you to implement much more powerful rendering techniques in the future.

This assignment consists of three parts. In the first, you will implement sampling routines for different distributions on the sphere. Then you will get to implement Monte Carlo sampling techniques for your existing materials. Finally, you will implement _integrators_ that will allow you to render your scene with different rendering algorithms.

As usual, begin by importing the latest base code updates into your repository using the command: `git pull` on the command-line, or the equivalent version if you are using a GUI. You should also get a copy of the text book _Ray Tracing: The Rest of Your Life_ and read chapters 0-6.

Sampling Distributions (3 _pts_)
========================================

In this task, you will start with the basics of generating points on the sphere that have specific distributions. We recommend you read _Ray Tracing: The Rest of Your Life_ up to and including chapter 5 before you start.

To help you get a feel for what these different distributions look like, we want you to generate 3D visualizations of your point sets. For each of the distributions you will implement, write out a list of points to a text file and make e.g. a 3D scatter plot of the points. You can do this in Matlab or your favorite plotting software, or in one of many online plotting tools.

If you look at the very bottom of `common.h`, you can see that we already provide you with two functions for generating points in a circle and points in a sphere. These are generated using rejection sampling. To help you get started setting up your plotting tools, generate a few hundred points using `randomInUnitSphere()` and visualize them in your plotting tool. For reference, we will show you images of our points sets, which we generated by writing out a CSV file from C++ and rendering them using [plot.ly](https://chart-studio.plot.ly/create/#/) (click on "import" to upload a CSV, and click on +Trace, type->3D Scatter to get a scatter plot). We generated the CSV by adding a new program in dirt called `03_point_gen` with the following source code:

![](https://canvas.dartmouth.edu/courses/48965/files/8415703/preview)

To compile this new program, type your source code in a new `.cpp` file (we called it `03_point_gen`) and modify the `CMakeLists.txt` file to tell your compiler about your new executable:

![](https://canvas.dartmouth.edu/courses/48965/files/8415702/preview)

You can then run this program from the terminal as before. Hint: You can type `build/03_point_gen > points.csv` in the terminal to directly save the output of the program into a CSV file.

If you implemented everything correctly, your program should generate a point distribution like this:

![](https://canvas.dartmouth.edu/courses/48965/files/8415704/preview)

This is only for inspiration: You don't have to do it this way, and are free to generate your points in any way you want and visualize them in any software that is easiest for you. Your output will differ depending on which plotting software you use, how many points you output and so forth, but it should look like uniform points within a sphere.

### Uniform Points on the Sphere

Add a new function called `randomOnUnitSphere()` that generates points _on_ the sphere instead of its interior. You can begin by simply calling `randomInUnitSphere()` and normalizing the result: That will project all points to the surface of the sphere, and give you a result like this:

![](https://canvas.dartmouth.edu/courses/48965/files/8415705/preview)

While this gives you the distribution you want, it is inefficient: Since `randomInUnitSphere()` uses rejection sampling, it could loop many times before giving you a sample. Instead, write a function that directly produces points on the sphere. [WolframAlpha](http://mathworld.wolfram.com/SpherePointPicking.html) shows several ways to do this. The easiest is the trigonometric way, for which we give pseudocode below:

~~~ C++
phi = randf()*2*pi;
cosTheta = 2*randf() - 1;
sinTheta = sqrt(1 - cosTheta*cosTheta);
x = cos(phi)*sinTheta;
y = sin(phi)*sinTheta;
z = cosTheta;
return vector(x, y, z);
~~~

After you implement the function without rejection sampling, rerun it and visualize the point set. It should give you the same distribution as before, except now you don't rely on rejection sampling.

### Uniform Points on the Hemisphere

Add a new function called `randomOnUnitHemisphere()` that generates points on the hemisphere (i.e. the parts of the sphere with positive z coordinated) instead of the full sphere. You can achieve this by modifying the way `cosTheta` is computed in the previous code for sampling the sphere to only produce positive z values, i.e. you can change it to `cosTheta = randf()`.

This should give you a distribution like this:

![](https://canvas.dartmouth.edu/courses/48965/files/8415706/preview)

### Cosine-weighted points on the Hemisphere

Add a new function called `randomCosineHemisphere()`. This function should generate points so that the PDF of the points is proportional to the cosine of the angle between the point and the z-axis. You can again achieve this by modifying the way `cosTheta` is computed in the previous functions: `cosTheta = sqrt(randf())`.

This should give you a distribution like this:

![](https://canvas.dartmouth.edu/courses/48965/files/8415707/preview)

### Cosine-power-weighted points on the Hemisphere

Add a new function called `randomCosinePowerHemisphere(float exponent)`. This function should generate points so that the PDF of the points is proportional to the cosine of the angle between the point and the z-axis, _raised to the power of_ some exponent. For an exponent of 1, we should get standard cosine-weighted hemisphere sampling; for an exponent of 0, we should get the uniform hemisphere, and the distribution should get more and more clustered around (0, 0, 1) for higher and higher exponents.

You can again achieve this by modifying the way `cosTheta` is computed in the above code. Change it to `cosTheta = powf(randf(), 1/(exponent + 1))`.

Try to insert `exponent=0` and `exponent=1` into the above equation. Does it do what we want it to do?

Write out a point set for `exponent=20`. This should give you a distribution like this:

  ![](https://canvas.dartmouth.edu/courses/48965/files/8415708/preview)

Note how clustered the points are around the z-axis.

Sampling Materials (4 _pts_)
====================================

In this task, you will redesign your material class to support three new queries. The first of these is `eval()`: This method will evaluate how much light flows between a given incoming- and outgoing ray direction. This method entirely determines what the material will look like.

The other two queries are needed for rendering materials using Monte Carlo. The `sample()` method takes an incoming ray direction and samples an outgoing ray direction. The `pdf()` function returns the probability density of the ray directions produced by `sample()`. These two queries need to be exactly matched to each other - i.e. `pdf()` should actually be the exact distribution of samples coming out of `sample()` - but they do not have to match the shape of the material reflectance exactly (i.e. `pdf()` does not have to match `eval()`). It helps if it does - you will have less noise that way - but it does not have to. This allows you to render materials that cannot be sampled exactly. This is a big departure from Peter Shirley's books, in which it is implicitly assumed that the sampling method matches the material reflectance exactly, and changing the `scatter` method would change both the sampling strategy and the appearance of the material.

Begin by adding new virtual methods for each of the three queries to the `Material` base class in `material.h`. You can design these any way you would like, but to help you get started, we give you the interfaces we use in our code for inspiration:

`eval()`:

~~~ C++
/*
    Evaluate the material response for the given pair of directions.

    For non-specular materials, this should be the BSDF multiplied by the
    cosine foreshortening term.

    Specular contributions should be excluded.
*/
virtual Color3f eval(const Vec3f & dirIn, const Vec3f & scattered, const HitInfo & hit) const 
{
    return Color3f(0.0f);
}
~~~

`pdf()`:

~~~ C++
/**
    Compute the probability density that #sample() will generate #scattered (given #dirIn).
    
    \param  dirIn      The incoming ray direction
    \param  scattered  The outgoing ray direction
    \param  hit        The shading hit point
    \return float      A probability density value in solid angle measure around #hit.p.
    */
virtual float pdf(const Vec3f & dirIn, const Vec3f & scattered, const HitInfo & hit) const 
{
    return 0.0f;
}
~~~

`sample()`:

~~~ C++
/**
    Sample a scattered direction at the surface hitpoint \c hit.

    If it is not possible to evaluate the pdf of the material (e.g.\ it is
    specular or unknown), then set srec.isSpecular to true, and populate
    srec.scattered and srec.attenuation just like we did previously in the
    \ref scatter() function. This allows you to fall back to the way we did
    things with the \ref scatter() function, i.e.\ bypassing \ref pdf()
    evaluations needed for explicit Monte Carlo integration in your \ref
    Integrator, but this also precludes the use of MIS or mixture sampling
    since the pdf is unknown.

    \param  dirIn  The incoming ray direction
    \param  hit    The incoming ray's intersection with the surface
    \param  srec   Populate srec.scattered, srec.isSpecular, and
                    srec.attenuation
    \return bool   True if the surface scatters light
*/
virtual bool sample(const Vec3f & dirIn, const HitInfo &hit, ScatterRecord &srec) const 
{
    return false;
}
~~~

You can see that `sample()` does not return the scattered direction, but instead fills out a `ScatterRecord` structure. Our structure looks as follows:

~~~ C++
struct ScatterRecord
{
    Color3f attenuation;         ///< Attenuation to apply to the traced ray
    Vec3f   wo;                  ///< The sampled outgoing direction
    bool    is_specular = false; ///< Flag indicating whether the ray has a degenerate PDF
};
~~~

This might look strange: In theory, all that `sample()` needs to generate is a scattered direction. However, there are two additional fields: `attenuation` and `isSpecular`. The reason for this is that certain materials (like mirrors or glass) do not have real PDFs, or have PDFs that are very difficult to compute (like Peter Shirley's metal material). Those materials will simply call the `scatter()` methods we implemented previously to fill out `attenuation` and `scattered`, and will set the `isSpecular` flag to signal to the rest of the code that this material does not have a PDF and we should fall back to the non-Monte Carlo way of computing colors.

For now, implement the `sample()` method for all your existing materials and make it a) set `isSpecular` to true and b) call the existing `scatter` method to fill out `srec.scattered` and `srec.attenuation`. This fallback will make sure your existing materials are backwards compatible. In the following tasks, you will slowly replace the fallback with real sampling code and PDFs.

Lambertian Sampling
-------------------

The formula for the Lambertian material is extremely simple: It only depends on the normal and the scattered direction, and does not take the incoming direction into account. In pseudo-code, it is `albedo * max(0, dot(scatteredDir, normal))/pi`.

Implement this as the `eval()` method of your `Lambertian` material.

The PDF of a `Lambertian` material is even simpler: It is `max(0, dot(scatteredDir, normal))/pi`.

Implement this as the `pdf()` method of the Lambertian material.

The only remaining problem is to sample this PDF. However, you already implemented the bulk of the sampling code in task 1, when you implemented `randomCosineHemisphere()`. All you need to do is to modify the `sample()` method to set `srec.scattered` to the random cosine-weighted direction. Since you now have proper sampling code, remember to set `isSpecular` to false!

Testing your code
-----------------

Testing sampling code can be notoriously difficult. To help you with this, we wrote a small sample tester utility. This tool will run your sampling code and output statistics to help you decide whether your implementation is correct or not.

You can get the sample test utility [in this github gist](https://gist.github.com/tunabrain/b1e2a729021767f79f6487426cbeea6b). Note that this code assumes you use the same sampling interface as we use - if you made changes, you might have to adapt the tool. You will have to tell CMake about this new executable; modify your CMakeLists to do this, just like you did in Task 1.

Once you managed to compile the sample test, go to `main()` and uncomment the first `runTest()` statement. This will test your Lambertian material.

The tester will do two things: First, it will evaluate your PDF over the sphere and output its integral. This integral should be close to 1 (otherwise, it is not a PDF). You should see an output like

Integral of PDF (should be close to 1): 1.00003

Second, the code will call `sample()` many times and build a histogram of the sampled directions. This histogram should exactly match the PDF. The tester will output two images, `lambertian-pdf.png` (the PDF) and `lambertian-sampled.png` (the histogram). These should match with each other and look similar to this:

![](https://canvas.dartmouth.edu/courses/48965/files/8415710/preview width="95%")   ![](https://canvas.dartmouth.edu/courses/48965/files/8415711/preview width="95%")

Additionally, your sample method should return successfully and not generate invalid directions; the code will test for that and output a statement like

100% of samples were valid (this should be close to 100%)

Make sure your code passes the tests before proceeding. If you have any problems, make sure you normalize the directions that are passed into your code - sometimes the `scattered`/`dirIn` directions might not have unit length, which will make your code misbehave.

Oriented Samples
----------------

So far, your Lambertian sampling code has a big problem: It assumes the normal is (0, 0, 1). This makes the sampling algorithm simpler, but - of course - most of the time, the surface normal will not be (0, 0, 1).

To fix this, read chapter 6 of the text book and implement an _ONB_ (Ortho-Normal Basis) class. This will allow you to rotate directions for arbitrary normals. Modify your `sample()` code to transform the sampled direction from `randomCosineHemisphere()` using the ONB, oriented to the surface normal. You can look at Peter Shirley's code for inspiration.

You can test your code using the `"rotated-lambertian"` test in the sample test tool. You should get images like these:

![](https://canvas.dartmouth.edu/courses/48965/files/8415648/preview width="95%")   ![](https://canvas.dartmouth.edu/courses/48965/files/8415716/preview width="95%")


Phong Material
--------------

Next to Lambert, the Phong reflection model is one of the earliest and simplest in computer graphics. It tries to model glossy reflections, such as from rough metal or plastic. Start by adding a new material in `material.h` called `Phong`. Modify `parser.cpp` to create this material when you encounter the `"phong"` material type.

For a perfect mirror material (such as polished metal), all light scatters into the mirror direction, i.e. the incoming ray reflected by the normal. The `Phong` material tries to approximate surface roughness by "blurring" the reflection around the normal, so that light is spread around the perfect mirror direction. The amount of spread can be controlled by a parameter to model different amounts of roughness of the surface. In the `Phong` material, this spread is called the `exponent`. Add an `exponent` member to your `Phong` class, and get it from JSON in the `Phong` constructor.

The PDF of the `Phong` material looks like this (pseudocode):

~~~ C++
mirrorDir = normalize(reflect(dirIn, normal));
cosine = max(dot(normalize(scattered), mirrorDir), 0);
return constant*powf(cosine, exponent);
~~~

It measures the cosine of the angle between the scattered direction and the perfect mirror direction. The cosine is then raised to a power. For low exponents, there is a wide spread of this PDF around the mirror direction; for large exponents, the PDF becomes more and more concentrated around the mirror direction. `constant` is a normalization constant to make sure the PDF integrates to 1. It is computed like this: `constant = (exponent + 1)/(2*pi)`.

The result of `eval()` is simply the PDF multiplied by the albedo.

Sampling this material follows in two steps: First, you need to generate directions from a distribution proportional to a cosine raised to a power (Hint: You already implemented this; it is `randomCosinePowerHemisphere(exponent)`). Second, you need to orient those samples so they are centered on the mirror direction (Hint: Use your ONB class, centered on the reflected incoming direction). Note that this can generate directions below the hemisphere, so make sure to reject those (i.e. return false in `sample()`)!

You can test your code using the `"phong"` and `"rotated-phong"` tests in the sample test tool. You should get images like these:

![](https://canvas.dartmouth.edu/courses/48965/files/8415712/preview width="95%")   ![](https://canvas.dartmouth.edu/courses/48965/files/8415713/preview width="95%")
![](https://canvas.dartmouth.edu/courses/48965/files/8415717/preview width="95%")   ![](https://canvas.dartmouth.edu/courses/48965/files/8415718/preview width="95%")

Grad Students Only: Blinn-Phong
-------------------------------

If you are a grad student, you will also need to implement the _Blinn-Phong_ shading model, which is a slightly more realistic variation on the standard Phong model. In Blinn-Phong, we first generate a _random normal_, and then treat that normal as the surface normal for reflecting the incoming direction. Similar to Phong, this will create random directions distributed around the mirror direction, but with a different shape. Begin by creating a new `BlinnPhong` material that also has an `exponent` parameter, just like Phong. Modify the parser to create this new material for the `"blinnphong"` key.

The easiest method to implement is the `sample()` method, which we will start with. It should use `randomCosinePowerHemisphere()` and your `ONB` class to generate a cosine-power weighted direction centered on the surface normal. Use this new, random direction to reflect the incoming ray (as if the random direction was the real surface normal). Note that this can generate directions below the hemisphere, so make sure to reject those (i.e. return false in `sample()`)!

The PDF method will be slightly more complicated. You first need to figure out which random normal would reflect the (given) incoming ray so that it matches the scattered ray. You can use the fact that the normal must lie exactly halfway between the incoming and scattered direction:

~~~ C++
randomNormal = normalize(-normalize(dirIn) + normalize(scattered))
~~~

Then, you need to compute the PDF of generating this random normal. Since we generated this from a cosine-power distribution, this must be the cosine-power PDF:

~~~ C++
cosine = max(dot(randomNormal, realNormal), 0);
normalPdf = (exponent + 1)/(2*pi) * powf(cosine, exponent);
~~~

Finally, we need to account for a warping of the PDF by the reflection operation. The final PDF becomes:

~~~ C++
finalPDF = normalPdf/(4*dot(-dirIn, randomNormal))
~~~

The `eval()` method is simply the PDF method times the albedo.

You can test your implementation using the sample tester tool. You should get images like these:

![](https://canvas.dartmouth.edu/courses/48965/files/8415611/preview width="95%")   ![](https://canvas.dartmouth.edu/courses/48965/files/8415709/preview width="95%")
![](https://canvas.dartmouth.edu/courses/48965/files/8415714/preview width="95%")   ![](https://canvas.dartmouth.edu/courses/48965/files/8415715/preview width="95%")

!!! Tip: Hint
    Not all Blinn-Phong directions make sense. If the incoming or scattered directions are below the hemisphere, the PDF should be zero.

!!! Warning:
    Sampling Blinn-Phong can generate quite a few directions below the hemisphere. Your valid sample count might go as low as 90%, and the integral of your PDF might go as low as 0.9.

Integrators (4 _pts_)
=================================

In this task, you will extend your renderer to support different _integrators._ In computer graphics, we say "integrator" to refer to an algorithm for rendering an image. So far, we have been using the algorithm outlined in Peter Shirley's books, but there are more powerful (and less noisy) integrators out there, and you will implement the ability to select different integrators in this task.

Begin by creating an `Integrator` base class (you could do this in a new header file called `integrator.h`. This class should have a method for returning the color along a particular ray similar to `recursiveColor` that we used in previous assignments. You can call this method what you would like, but we named it `Li` for "Incoming radiance" or "light, incoming". You can add parameters to this method as you see fit, but for inspiration, this is what our method looks like:

![Li method](https://canvas.dartmouth.edu/courses/48965/files/8415661/preview)

Normal Integrator
-----------------

To test your new integrator class, create a test integrator class called `NormalIntegrator` that inherits from `Integrator` and simply renders the normals of the visible geometry in the scene. In `NormalIntegrator::Li`, call `scene.intersect` to intersect the scene along the ray passed to `Li` and, if you hit something, return the _absolute_ value of the shading normal as the output color. If you do not hit anything, return black.

Now that we have a test integrator, we need to do some plumbing to make the scene use our new integrator interface. In the scene class (in `scene.h`), add a `shared_ptr&lt;Integrator&gt; m_integrator;` field. This will contain the integrator used for rendering the image. Go to `scene.cpp` and change `Scene::raytrace` to call `m_integrator->Li(*this, ...)` instead of `recursiveColor(...)`. To remain backwards compatible with scenes files that don't specify an integrator, you could first check if `m_integrator` is null, and use the old `recursiveColor` if it is.

Now add the ability to set the integrator from JSON. In `parser.cpp`, go to `Scene::parseFromJSON`. This function deciphers keywords in the json and creates corresponding scene objects. Take inspiration from the way `parseFromJSON` sets the `m_camera` member of the scene and add a new if statement that checks for an `"integrator":` field and sets the `m_integrator` member of the scene. We want to support many different kinds of integrators, so add a `parseIntegrator` function that looks at the `"type"` to create the appropriate integrator (much like `parseTexture` from the previous assignment), and call this new function to set the `m_integrator` field. You already implemented one integrator, so make `parseIntegrator` create an instance of your `NormalIntegrator` if the type is `"normals"`.

You can now test your implementation by running it on `scenes/03_mc_integration/ajax-normals.json`. If everything works correctly, you should get an image like this:

![Ajax rendered with normal integrator](https://canvas.dartmouth.edu/courses/48965/files/8415660/preview)

Ambient Occlusion Integrator
----------------------------

Now that the machinery for different integrators is in place, we can start creating more interesting integrators. Create a new `AmbientOcclusionIntegrator` class that will generate an _ambient occlusion_ image. Ambient occlusion is a rendering technique which assumes that a (diffuse) surface receives uniform illumination from all directions (similar to the conditions inside a [light box](https://en.wikipedia.org/wiki/Lightbox#/media/File:DIY_Lightbox.jpg)), and that visibility is the only effect that matters. Some surface positions will receive less light than others since they are occluded, hence they will look darker. Modify `parseIntegrator` to create an `AmbientOcclusionIntegrator` for the integrator type `"ao"`.

Implementing ambient occlusion is very simple: In your integrator's Li method, first find the intersection along the passed in camera ray. If the ray misses the scene, return black; if it hits a surface, sample the material at that point (use your new sample method for this). Now, construct a new "shadow" ray starting at the hit location going in the scattered direction, and check if it intersects something. If this new ray does not intersect anything, it means this ray sees the sky: Return white in this case. If the ray hits something, the sky is occluded, and you should return black instead.

You can test your implementation by running it on `scenes/03_mc_integration``/ajax-ao.json`. If everything works correctly, you should get an image like this:

![Ajax rendered with the AmbientOcclusionIntegrator](https://canvas.dartmouth.edu/courses/48965/files/8415656/preview)

Material Sampling Integrator
----------------------------

The algorithm described by Peter Shirley in his books is (secretly) a form of _path tracing_, which is a rendering algorithm that traces rays exclusively from the camera. The flavor of path tracing you have been implementing in the previous assignments is sometimes called "Kajiya-style path tracing", after its inventor.

In this task, you will reimplement this flavor of path tracing as its own integrator, but this time you will make the PDFs and sampling methods that are involved explicit. This will allow you to implement more powerful forms of path tracing in the future.

Begin by making a new `PathTracerMaterials` integrator. This integrator will start from a camera ray and recursively trace new rays by sampling materials. Write your `Li` method based on the `Scene::recursiveColor` method you wrote in previous assignments. Instead of calling `Material::scatter` and multiplying recursive calls by `attenuation`, call your new `Material::sample` method, and multiply recursive calls by `Material::eval(...)/Material::pdf(...)`. Implicitly, that is what Peter Shirley's algorithm was already doing: `attenuation` was `eval/pdf` in disguise, with factors appearing in both terms already cancelled out.

The only wrinkle in this is that some materials might not support the PDF interface (such as mirrors/glass or Peter Shirley's metal material). You can detect this by checking the `isSpecular` flag; if that is set, multiply by `attenuation` instead of `eval/pdf`.

You can test your implementation by running it on `scenes/03_mc_integration/phong.json` or `scenes/03_mc_integration/buddha-box.json`.

![](https://canvas.dartmouth.edu/courses/48965/files/8415719/preview)

![](https://canvas.dartmouth.edu/courses/48965/files/8415720/preview)

What to submit
==============

**Please read the following instructions carefully: Include the required files and nothing else. Do not pack the required files into a containing folder, just zip the files directly.**

Please edit the `report/report.html` report template and fill in your feedback, comments and optional explanations. Add a section for each task. If a task provides a reference image, please include a comparison between the image produced by your code and the reference (the template shows you how to do this easily).

Upload a zip file containing

*   The source code (the entire `include/` and `src/` folders, as well as your `CMakeLists.txt`)
*   Visualized point sets for all distributions from task 1
*   Rendered images of all the scenes in `scenes/03_mc_integration`
*   The `report` folder.

**Do not submit any binaries. Do not include your build folder in the zip file. Do not put the required files/folders into a containing folder; zip them directly and submit the zip file.**

<script>
markdeepOptions = {tocStyle: 'medium'};
</script>


<script src="../js/common.js"></script>
